{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH05UOEEQUYV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from google.colab import output\n",
    "import cv2 as cv\n",
    "\n",
    "import IPython\n",
    "from IPython import display\n",
    "from IPython.display import Javascript, Image\n",
    "\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "from google.colab.output import eval_js\n",
    "import base64\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1616278074065,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -60
    },
    "id": "DUa4sgbwSLmB",
    "outputId": "9157735a-4e61-4aaa-f991-73f07a8bc996"
   },
   "outputs": [],
   "source": [
    "# Wir machen google-drive dem Notebook bekannt, um Zugriff auf die Daten zu erhalten\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_dir = '/content/drive/MyDrive/hackerschool/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBMFK4xNQvmC"
   },
   "outputs": [],
   "source": [
    "# Erst wird die Struktur des Models erstellt\n",
    "emotion_detector = Sequential()\n",
    "\n",
    "emotion_detector.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_detector.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_detector.add(Dropout(0.25))\n",
    "\n",
    "emotion_detector.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_detector.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_detector.add(Dropout(0.25))\n",
    "\n",
    "emotion_detector.add(Flatten())\n",
    "emotion_detector.add(Dense(1024, activation='relu'))\n",
    "emotion_detector.add(Dropout(0.5))\n",
    "emotion_detector.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtMp3d5BQ_AW"
   },
   "outputs": [],
   "source": [
    "# Jetzt müssen wir die Gewichte vom Model laden, damit wir es tatsächlich verwenden können\n",
    "model_weights = os.path.join(base_dir, 'emotion-detector.h5')\n",
    "emotion_detector.load_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e979yEx2RaWD"
   },
   "outputs": [],
   "source": [
    "def Detect_Emotion(webcam_frame):\n",
    "  \n",
    "  # Der aktuelle Frame der WebCam wird aus dem Parameter gelesen und in ein Numpy-Array übertragen \n",
    "  binary = base64.b64decode(webcam_frame.split(',')[1])            \n",
    "  frame = cv.imdecode(np.frombuffer(binary, np.uint8), -1)\n",
    "    \n",
    "  # Da in Opencv mit BGR-Bilder gearbeitet wird, muss das Bild nach RGB übertragen werden\n",
    "  frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "  # Hier wird die Kaskade zur Erkennung von frontalen Gesichtern geladen \n",
    "  face_cascade = cv.CascadeClassifier()\n",
    "  face_cascade.load(os.path.join(base_dir,'haarcascade_frontalface_alt.xml'))\n",
    "    \n",
    "  # Das RGB-Bild muss für den Algorithmus noch zu einem Schwarz/Weiß-Bild konvertiert werden\n",
    "  frame_gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "  frame_gray = cv.equalizeHist(frame_gray)\n",
    "    \n",
    "  # Es wird nach Gesichtern in dem grauen Bild gesucht\n",
    "  faces = face_cascade.detectMultiScale(frame_gray)\n",
    "    \n",
    "  # Es wird über die Koordinaten von den erkannten Gesichtern iteriert\n",
    "  for (x,y,w,h) in faces:\n",
    "    # Hier wird ein Rechteck in Grün um das Gesicht gezeichnet\n",
    "    cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255,0), 2)\n",
    "    # Es wird eine Region of Interest (roi) im grauen und im bunten Bild gesetzt\n",
    "    # Damit wird der zu betrachtene Ausschnitt auf das Gesicht reduziert, denn in \n",
    "    # diesem Bereich vermuten wir weitere Bestandteile des Gesichts. \n",
    "    roi_gray = frame_gray[y:y+h, x:x+w]\n",
    "    \n",
    "    emotion = find_emotion(roi_gray)\n",
    "\n",
    "    cv.putText(frame, emotion, (x, y), cv.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 5, cv.LINE_AA)\n",
    "\n",
    "        \n",
    "  return compress_to_bytes(frame)\n",
    "  \n",
    "\n",
    "output.register_callback('notebook.Detect_Emotion', Detect_Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXEeuAjwqTHf"
   },
   "outputs": [],
   "source": [
    "# schreibt eine Methode, die das Modell aufruft und euch die aktuelle Emotion zurück gibt\n",
    "def find_emotion(roi_gray):\n",
    "  global emotion_detector\n",
    "  roi_gray = cv.resize(roi_gray, (48,48))\n",
    "  prepared = np.expand_dims(np.expand_dims(roi_gray, axis=-1), axis=0)\n",
    "  prediction = emotion_detector.predict(prepared)    \n",
    "  emotion = np.argmax(prediction)  \n",
    "  emotion = filter_emotion(emotion)\n",
    "  return emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRZ-OmB5SQCg"
   },
   "outputs": [],
   "source": [
    "def compress_to_bytes(data, fmt='png'):\n",
    "    \"\"\"\n",
    "    In dieser Hilfsfunktion wird das Bild in Bytes tranformiert und anschließend\n",
    "    als in ein JSON-Objekt übertragen, damit das Bild and die JavaScript-Funktion \n",
    "    zurück gesendet werden kann\n",
    "    \"\"\"\n",
    "    buff = io.BytesIO()\n",
    "    img = PIL.Image.fromarray(data)    \n",
    "    img.save(buff, format=fmt)\n",
    "    \n",
    "    data_encode = base64.b64encode(buff.getvalue())\n",
    "    \n",
    "    return IPython.display.JSON({'result': data_encode.decode(encoding='utf-8')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzZTLLsXTktu"
   },
   "outputs": [],
   "source": [
    "def filter_emotion(emotion):\n",
    "  emotions = [\"Angst\", \"Ekel\", \"Froh\", \"Neutral\", \"Traurig\", \"Überrascht\", \"Wütend\"]\n",
    "  return emotions[emotion]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xj1su9cdSXAe"
   },
   "outputs": [],
   "source": [
    "def start_webcam():\n",
    "\n",
    "  js = Javascript('''\n",
    "      (async function() {\n",
    "\n",
    "            // Wir erstellen Html-Elemente, welche den Input der WebCam zeigen\n",
    "            var video = document.createElement('video');        \n",
    "            video.id = 'video_stream'\n",
    "            video.style.display = 'hidden';\n",
    "            \n",
    "            var div = document.createElement('div');\n",
    "            div.id = 'video_container';   \n",
    "\n",
    "            // Mit diesem Button können wir die WebCam wieder ausschalten\n",
    "            stop_button = document.createElement('button');\n",
    "            stop_button.id = 'stop_button';\n",
    "            stop_button.innerHTML = 'Stop Video';     \n",
    "\n",
    "            div.appendChild(video);\n",
    "            div.appendChild(stop_button);\n",
    "                          \n",
    "            document.body.appendChild(div);\n",
    "\n",
    "            // Es wird die WebCam abgerufen, damit diese benutzt werden kann\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});      \n",
    "            video.srcObject = stream;\n",
    "\n",
    "            // Es wir darauf gewartet, dass die WebCam einsatzbereit ist\n",
    "            await video.play(); \n",
    "            \n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "            \n",
    "            // Das Video der WebCam wird auf eine Canvas gezeichnet\n",
    "            var canvas = document.createElement('canvas');\n",
    "            canvas.id = 'video_canvas';\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            \n",
    "            // Jede Sekunde wird die Detect-Methode aufgerufen, um das Gesicht zu erkennen.\n",
    "            let triggerID = setInterval(async function call_detect() {\n",
    "                \n",
    "                canvas.getContext('2d').drawImage(video, 0, 0);             \n",
    "                content = canvas.toDataURL('image/jpeg', 0.8);\n",
    "\n",
    "                const result = await google.colab.kernel.invokeFunction(\n",
    "                    'notebook.Detect_Emotion', // Name der Methode.\n",
    "                    [content], // Der aktuelle Frame der WebCam ist der Parameter für die Detect-Methode.\n",
    "                    {}); // kwargs\n",
    "\n",
    "                // Das Resultat der Detect-Methode wird wieder ausgelesen\n",
    "                img_url = result.data['application/json']['result'];\n",
    "                \n",
    "                // Um das Ergebnis der Detect-Methode anzuzeigen, erstellen wir ein neues Image\n",
    "                detected_image = new Image();\n",
    "                detected_image.onload = function () {{\n",
    "\n",
    "                    var image_canvas = document.getElementById('image_canvas');\n",
    "\n",
    "                    if (image_canvas == null) {                          \n",
    "                        image_canvas = document.createElement('canvas');\n",
    "                        image_canvas.id = 'image_canvas';                  \n",
    "                        document.body.appendChild(image_canvas)\n",
    "                    }      \n",
    "                    image_canvas.width = detected_image.width;\n",
    "                    image_canvas.style.width = detected_image.width + 'px';\n",
    "\n",
    "                    image_canvas.height = detected_image.height;\n",
    "                    image_canvas.style.height = detected_image.height + 'px';\n",
    "                    \n",
    "                    image_canvas.style.display = 'block';\n",
    "                    \n",
    "                    // das Bild wird auf die neue Canvas gezeichnet\n",
    "                    image_canvas.getContext('2d').drawImage(detected_image, 0, 0);\n",
    "\n",
    "                  }};\n",
    "\n",
    "                  // Das Resultat der Detect-Methode ist das Bild mit erkanntem Gesicht im Byte-Format.\n",
    "                  // Dieses Byte-Array wird als Quelle für das Bild angegeben, damit es dargestellt werden kann.\n",
    "                  detected_image.src = 'data:image/png;charset=utf-8;base64,'+img_url;\n",
    "              \n",
    "              } , 1000);\n",
    "\n",
    "            // Sobald der Button geklickt wird, wird die WebCam ausgeschaltet und die Detect-Methode\n",
    "            // nicht mehr aufgerufen. \n",
    "            stop_button.onclick = function() {                  \n",
    "                  stream.getVideoTracks()[0].stop();\n",
    "                  clearTimeout(triggerID);\n",
    "          };\n",
    "\n",
    "      })();\n",
    "    ''')\n",
    "\n",
    "  display.display(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1616279052224,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -60
    },
    "id": "iP5DMyLKSaUG",
    "outputId": "154db582-dae5-459a-dd02-83551b48aa38"
   },
   "outputs": [],
   "source": [
    "# Mit diesem Methoden-Aufruf starten wir die WebCam.\n",
    "start_webcam()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvjtx4sjmGiX+75m2nBPO1",
   "collapsed_sections": [],
   "mount_file_id": "1V5DiWy_DaKLSViM6D_BzRcTCJrpqdSEv",
   "name": "webcam_emotion_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
